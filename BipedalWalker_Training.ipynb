{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BipedalWalker-v3 Reinforcement Learning Training\n",
    "\n",
    "Обучение агента BipedalWalker с использованием алгоритмов SAC, TD3, PPO\n",
    "\n",
    "**Перед запуском:**\n",
    "1. Runtime -> Change runtime type -> T4 GPU\n",
    "2. Запускайте ячейки последовательно"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Проверка GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Установка зависимостей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Установка системных зависимостей для Box2D\n",
    "!apt-get update -qq > /dev/null 2>&1\n",
    "!apt-get install -y -qq swig build-essential python3-dev > /dev/null 2>&1\n",
    "\n",
    "# Установка Box2D отдельно\n",
    "!pip install -q box2d-py\n",
    "\n",
    "# Установка остальных пакетов\n",
    "!pip install -q stable-baselines3 gymnasium pygame tensorboard imageio imageio-ffmpeg\n",
    "\n",
    "print(\"✓ Все пакеты установлены!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Импорты и настройка"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import torch\n",
    "from stable_baselines3 import SAC, TD3, PPO\n",
    "from stable_baselines3.common.callbacks import EvalCallback, CheckpointCallback\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "import matplotlib.pyplot as plt\n",
    "import imageio\n",
    "import os\n",
    "from datetime import datetime\n",
    "from IPython.display import Video, HTML, display\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Проверка GPU\n",
    "print(f\"CUDA доступна: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Память GPU: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "else:\n",
    "    print(\"⚠️ GPU не обнаружена! Используется CPU.\")\n",
    "\n",
    "# Создание директорий\n",
    "!mkdir -p models logs tensorboard_logs results videos\n",
    "print(\"\\n✓ Директории созданы\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Тестирование окружения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Тест окружения\n",
    "env = gym.make('BipedalWalker-v3')\n",
    "print(f\"Observation space: {env.observation_space}\")\n",
    "print(f\"Action space: {env.action_space}\")\n",
    "\n",
    "# Тестовый эпизод\n",
    "obs, info = env.reset()\n",
    "for _ in range(5):\n",
    "    action = env.action_space.sample()\n",
    "    obs, reward, done, truncated, info = env.step(action)\n",
    "    if done or truncated:\n",
    "        break\n",
    "env.close()\n",
    "print(\"\\n✓ Окружение работает!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Функции для тренировки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_env():\n",
    "    \"\"\"Создание окружения\"\"\"\n",
    "    env = gym.make('BipedalWalker-v3')\n",
    "    env = Monitor(env)\n",
    "    return env\n",
    "\n",
    "def train_agent(algorithm='SAC', timesteps=500000, save_freq=50000):\n",
    "    \"\"\"\n",
    "    Тренировка агента\n",
    "    \n",
    "    Args:\n",
    "        algorithm: 'SAC', 'TD3', или 'PPO'\n",
    "        timesteps: Количество шагов обучения\n",
    "        save_freq: Частота сохранения чекпоинтов\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Тренировка {algorithm}\")\n",
    "    print(f\"Шагов: {timesteps:,}\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    # Создание окружений\n",
    "    env = make_env()\n",
    "    eval_env = make_env()\n",
    "    \n",
    "    # Пути для сохранения\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    model_dir = f\"models/{algorithm.lower()}_{timestamp}\"\n",
    "    log_dir = f\"tensorboard_logs/{algorithm.lower()}_{timestamp}\"\n",
    "    \n",
    "    os.makedirs(model_dir, exist_ok=True)\n",
    "    os.makedirs(log_dir, exist_ok=True)\n",
    "    \n",
    "    # Коллбэки\n",
    "    eval_callback = EvalCallback(\n",
    "        eval_env,\n",
    "        best_model_save_path=f\"{model_dir}/best\",\n",
    "        log_path=f\"logs/{algorithm.lower()}\",\n",
    "        eval_freq=10000,\n",
    "        deterministic=True,\n",
    "        render=False,\n",
    "        n_eval_episodes=5\n",
    "    )\n",
    "    \n",
    "    checkpoint_callback = CheckpointCallback(\n",
    "        save_freq=save_freq,\n",
    "        save_path=f\"{model_dir}/checkpoints\",\n",
    "        name_prefix=f\"{algorithm.lower()}_model\"\n",
    "    )\n",
    "    \n",
    "    # Создание модели\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    print(f\"Используется устройство: {device}\\n\")\n",
    "    \n",
    "    if algorithm == 'SAC':\n",
    "        model = SAC(\n",
    "            'MlpPolicy', env, verbose=1,\n",
    "            learning_rate=3e-4, buffer_size=300000,\n",
    "            batch_size=256, gamma=0.99, tau=0.02,\n",
    "            tensorboard_log=log_dir, device=device\n",
    "        )\n",
    "    elif algorithm == 'TD3':\n",
    "        model = TD3(\n",
    "            'MlpPolicy', env, verbose=1,\n",
    "            learning_rate=1e-3, buffer_size=200000,\n",
    "            batch_size=100, gamma=0.99, tau=0.005,\n",
    "            tensorboard_log=log_dir, device=device\n",
    "        )\n",
    "    elif algorithm == 'PPO':\n",
    "        model = PPO(\n",
    "            'MlpPolicy', env, verbose=1,\n",
    "            learning_rate=3e-4, n_steps=2048,\n",
    "            batch_size=64, n_epochs=10,\n",
    "            gamma=0.99, gae_lambda=0.95, clip_range=0.2,\n",
    "            tensorboard_log=log_dir, device=device\n",
    "        )\n",
    "    \n",
    "    # Обучение\n",
    "    print(f\"Начало обучения...\\n\")\n",
    "    model.learn(\n",
    "        total_timesteps=timesteps,\n",
    "        callback=[eval_callback, checkpoint_callback],\n",
    "        progress_bar=True\n",
    "    )\n",
    "    \n",
    "    # Сохранение\n",
    "    final_model_path = f\"{model_dir}/final_model\"\n",
    "    model.save(final_model_path)\n",
    "    print(f\"\\n✓ Модель сохранена: {final_model_path}\\n\")\n",
    "    \n",
    "    env.close()\n",
    "    eval_env.close()\n",
    "    \n",
    "    return model, model_dir\n",
    "\n",
    "print(\"✓ Функции загружены\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Настройка параметров\n",
    "\n",
    "**Выберите количество шагов:**\n",
    "- 50000 - быстрый тест (~10-15 минут на алгоритм)\n",
    "- 500000 - полное обучение (~1-1.5 часа на алгоритм)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# НАСТРОЙТЕ ЭТО ЗНАЧЕНИЕ:\n",
    "TIMESTEPS = 50000  # Быстрый тест\n",
    "# TIMESTEPS = 500000  # Полное обучение (раскомментируйте для полного обучения)\n",
    "\n",
    "print(f\"Будет обучено: {TIMESTEPS:,} шагов на алгоритм\")\n",
    "print(f\"Примерное время на алгоритм: {TIMESTEPS//5000} минут\")\n",
    "print(f\"Всего для 3 алгоритмов: ~{TIMESTEPS//5000 * 3} минут\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Тренировка алгоритмов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1. SAC (Soft Actor-Critic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_sac, dir_sac = train_agent('SAC', timesteps=TIMESTEPS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2. TD3 (Twin Delayed DDPG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_td3, dir_td3 = train_agent('TD3', timesteps=TIMESTEPS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3. PPO (Proximal Policy Optimization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ppo, dir_ppo = train_agent('PPO', timesteps=TIMESTEPS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Функция для оценки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, algorithm_name, n_episodes=10, render_video=True):\n",
    "    \"\"\"\n",
    "    Оценка модели\n",
    "    \"\"\"\n",
    "    print(f\"\\nОценка {algorithm_name}...\")\n",
    "    env = gym.make('BipedalWalker-v3', render_mode='rgb_array')\n",
    "    \n",
    "    episode_rewards = []\n",
    "    frames = []\n",
    "    \n",
    "    for episode in range(n_episodes):\n",
    "        obs, info = env.reset()\n",
    "        done = False\n",
    "        truncated = False\n",
    "        episode_reward = 0\n",
    "        \n",
    "        while not (done or truncated):\n",
    "            action, _ = model.predict(obs, deterministic=True)\n",
    "            obs, reward, done, truncated, info = env.step(action)\n",
    "            episode_reward += reward\n",
    "            \n",
    "            if render_video and episode == 0:\n",
    "                frame = env.render()\n",
    "                if frame is not None:\n",
    "                    frames.append(frame)\n",
    "        \n",
    "        episode_rewards.append(episode_reward)\n",
    "        print(f\"  Эпизод {episode + 1}: {episode_reward:.2f}\")\n",
    "    \n",
    "    env.close()\n",
    "    \n",
    "    # Сохранение видео\n",
    "    video_path = None\n",
    "    if frames:\n",
    "        video_path = f'videos/{algorithm_name.lower()}_evaluation.mp4'\n",
    "        imageio.mimsave(video_path, frames, fps=30)\n",
    "        print(f\"  ✓ Видео: {video_path}\")\n",
    "    \n",
    "    # Статистика\n",
    "    mean_reward = np.mean(episode_rewards)\n",
    "    std_reward = np.std(episode_rewards)\n",
    "    \n",
    "    print(f\"\\n  Средняя награда: {mean_reward:.2f} ± {std_reward:.2f}\")\n",
    "    print(f\"  Диапазон: [{np.min(episode_rewards):.2f}, {np.max(episode_rewards):.2f}]\\n\")\n",
    "    \n",
    "    return episode_rewards, video_path\n",
    "\n",
    "print(\"✓ Функция оценки загружена\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Оценка моделей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Оценка всех моделей\n",
    "print(\"=\"*60)\n",
    "print(\"ОЦЕНКА МОДЕЛЕЙ\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "rewards_sac, video_sac = evaluate_model(model_sac, 'SAC', n_episodes=10)\n",
    "rewards_td3, video_td3 = evaluate_model(model_td3, 'TD3', n_episodes=10)\n",
    "rewards_ppo, video_ppo = evaluate_model(model_ppo, 'PPO', n_episodes=10)\n",
    "\n",
    "print(\"✓ Оценка завершена\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Просмотр видео"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Показать видео SAC\n",
    "if video_sac:\n",
    "    print(\"SAC:\")\n",
    "    display(Video(video_sac, embed=True, width=600))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Показать видео TD3\n",
    "if video_td3:\n",
    "    print(\"TD3:\")\n",
    "    display(Video(video_td3, embed=True, width=600))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Показать видео PPO\n",
    "if video_ppo:\n",
    "    print(\"PPO:\")\n",
    "    display(Video(video_ppo, embed=True, width=600))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Сравнение алгоритмов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# График сравнения\n",
    "algorithms = ['SAC', 'TD3', 'PPO']\n",
    "mean_rewards = [\n",
    "    np.mean(rewards_sac),\n",
    "    np.mean(rewards_td3),\n",
    "    np.mean(rewards_ppo)\n",
    "]\n",
    "std_rewards = [\n",
    "    np.std(rewards_sac),\n",
    "    np.std(rewards_td3),\n",
    "    np.std(rewards_ppo)\n",
    "]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "x_pos = np.arange(len(algorithms))\n",
    "colors = ['#1f77b4', '#ff7f0e', '#2ca02c']\n",
    "\n",
    "bars = plt.bar(x_pos, mean_rewards, yerr=std_rewards,\n",
    "               align='center', alpha=0.8, ecolor='black',\n",
    "               capsize=10, color=colors)\n",
    "\n",
    "plt.xlabel('Алгоритм', fontsize=12, fontweight='bold')\n",
    "plt.ylabel('Средняя награда', fontsize=12, fontweight='bold')\n",
    "plt.title('Сравнение производительности алгоритмов RL\\nBipedalWalker-v3', \n",
    "         fontsize=14, fontweight='bold')\n",
    "plt.xticks(x_pos, algorithms, fontsize=11)\n",
    "plt.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Значения над столбцами\n",
    "for bar, mean, std in zip(bars, mean_rewards, std_rewards):\n",
    "    height = bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width()/2., height,\n",
    "            f'{mean:.1f}±{std:.1f}',\n",
    "            ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('results/comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n✓ График сохранен: results/comparison.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Итоговая таблица"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Создание итоговой таблицы\n",
    "results_df = pd.DataFrame({\n",
    "    'Алгоритм': ['SAC', 'TD3', 'PPO'],\n",
    "    'Средняя награда': [np.mean(rewards_sac), np.mean(rewards_td3), np.mean(rewards_ppo)],\n",
    "    'Std': [np.std(rewards_sac), np.std(rewards_td3), np.std(rewards_ppo)],\n",
    "    'Мин': [np.min(rewards_sac), np.min(rewards_td3), np.min(rewards_ppo)],\n",
    "    'Макс': [np.max(rewards_sac), np.max(rewards_td3), np.max(rewards_ppo)]\n",
    "})\n",
    "\n",
    "results_df = results_df.round(2)\n",
    "results_df = results_df.sort_values('Средняя награда', ascending=False)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ИТОГОВЫЕ РЕЗУЛЬТАТЫ\")\n",
    "print(\"=\"*60)\n",
    "print(results_df.to_string(index=False))\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Сохранение\n",
    "results_df.to_csv('results/results.csv', index=False)\n",
    "print(\"\\n✓ Результаты сохранены: results/results.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Загрузка TensorBoard в Colab\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir tensorboard_logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Скачивание результатов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Архивирование результатов\n",
    "!zip -r -q results.zip models/ videos/ results/ logs/\n",
    "\n",
    "print(\"✓ Результаты упакованы в results.zip\")\n",
    "print(\"\\nСкачивание файла...\")\n",
    "\n",
    "# Скачивание\n",
    "from google.colab import files\n",
    "files.download('results.zip')\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"РАБОТА ЗАВЕРШЕНА!\")\n",
    "print(\"=\"*60)\n",
    "print(\"✓ Обучены 3 алгоритма: SAC, TD3, PPO\")\n",
    "print(\"✓ Созданы видео работы агентов\")\n",
    "print(\"✓ Построены графики сравнения\")\n",
    "print(\"✓ Результаты сохранены в results.zip\")\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
