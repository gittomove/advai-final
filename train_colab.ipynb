{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BipedalWalker RL Training\n",
    "\n",
    "Training SAC, TD3, and PPO agents on BipedalWalker-v3 environment.\n",
    "\n",
    "**Setup: Runtime → Change runtime type → T4 GPU**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!apt-get update -qq > /dev/null 2>&1\n",
    "!apt-get install -y -qq swig build-essential python3-dev > /dev/null 2>&1\n",
    "!pip install -q box2d-py stable-baselines3 gymnasium pygame tensorboard imageio imageio-ffmpeg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import torch\n",
    "from stable_baselines3 import SAC, TD3, PPO\n",
    "from stable_baselines3.common.callbacks import EvalCallback, CheckpointCallback\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "import matplotlib.pyplot as plt\n",
    "import imageio\n",
    "import os\n",
    "from datetime import datetime\n",
    "from IPython.display import Video, display\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"Device: {torch.cuda.get_device_name(0)}\")\n",
    "\n",
    "!mkdir -p models logs tensorboard_logs results videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training configuration\n",
    "TIMESTEPS = 500000\n",
    "SAVE_FREQ = 50000\n",
    "\n",
    "def make_env():\n",
    "    return Monitor(gym.make('BipedalWalker-v3'))\n",
    "\n",
    "def train(algorithm, timesteps=TIMESTEPS):\n",
    "    print(f\"\\n{'='*60}\\nTraining {algorithm} - {timesteps:,} steps\\n{'='*60}\\n\")\n",
    "    \n",
    "    env = make_env()\n",
    "    eval_env = make_env()\n",
    "    \n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    model_dir = f\"models/{algorithm.lower()}_{timestamp}\"\n",
    "    log_dir = f\"tensorboard_logs/{algorithm.lower()}_{timestamp}\"\n",
    "    os.makedirs(model_dir, exist_ok=True)\n",
    "    \n",
    "    eval_cb = EvalCallback(eval_env, best_model_save_path=f\"{model_dir}/best\",\n",
    "                           log_path=f\"logs/{algorithm.lower()}\", eval_freq=10000,\n",
    "                           deterministic=True, render=False, n_eval_episodes=5)\n",
    "    \n",
    "    checkpoint_cb = CheckpointCallback(save_freq=SAVE_FREQ, save_path=f\"{model_dir}/checkpoints\",\n",
    "                                       name_prefix=f\"{algorithm.lower()}_model\")\n",
    "    \n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    print(f\"Using device: {device}\\n\")\n",
    "    \n",
    "    if algorithm == 'SAC':\n",
    "        model = SAC('MlpPolicy', env, verbose=1, learning_rate=3e-4, buffer_size=300000,\n",
    "                   batch_size=256, gamma=0.99, tau=0.02, tensorboard_log=log_dir, device=device)\n",
    "    elif algorithm == 'TD3':\n",
    "        model = TD3('MlpPolicy', env, verbose=1, learning_rate=1e-3, buffer_size=200000,\n",
    "                   batch_size=100, gamma=0.99, tau=0.005, tensorboard_log=log_dir, device=device)\n",
    "    elif algorithm == 'PPO':\n",
    "        model = PPO('MlpPolicy', env, verbose=1, learning_rate=3e-4, n_steps=2048,\n",
    "                   batch_size=64, n_epochs=10, gamma=0.99, gae_lambda=0.95,\n",
    "                   clip_range=0.2, tensorboard_log=log_dir, device=device)\n",
    "    \n",
    "    model.learn(total_timesteps=timesteps, callback=[eval_cb, checkpoint_cb], progress_bar=True)\n",
    "    \n",
    "    model.save(f\"{model_dir}/final_model\")\n",
    "    print(f\"\\nModel saved: {model_dir}/final_model\\n\")\n",
    "    \n",
    "    env.close()\n",
    "    eval_env.close()\n",
    "    \n",
    "    return model, model_dir\n",
    "\n",
    "print(f\"Configuration: {TIMESTEPS:,} timesteps per algorithm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train SAC\n",
    "model_sac, dir_sac = train('SAC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train TD3\n",
    "model_td3, dir_td3 = train('TD3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train PPO\n",
    "model_ppo, dir_ppo = train('PPO')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation function\n",
    "def evaluate(model, name, n_episodes=10):\n",
    "    print(f\"\\nEvaluating {name}...\")\n",
    "    env = gym.make('BipedalWalker-v3', render_mode='rgb_array')\n",
    "    \n",
    "    rewards = []\n",
    "    frames = []\n",
    "    \n",
    "    for ep in range(n_episodes):\n",
    "        obs, _ = env.reset()\n",
    "        done = truncated = False\n",
    "        ep_reward = 0\n",
    "        \n",
    "        while not (done or truncated):\n",
    "            action, _ = model.predict(obs, deterministic=True)\n",
    "            obs, reward, done, truncated, _ = env.step(action)\n",
    "            ep_reward += reward\n",
    "            \n",
    "            if ep == 0:\n",
    "                frames.append(env.render())\n",
    "        \n",
    "        rewards.append(ep_reward)\n",
    "        print(f\"  Episode {ep + 1}: {ep_reward:.2f}\")\n",
    "    \n",
    "    env.close()\n",
    "    \n",
    "    video_path = f'videos/{name.lower()}.mp4'\n",
    "    imageio.mimsave(video_path, frames, fps=30)\n",
    "    \n",
    "    mean = np.mean(rewards)\n",
    "    std = np.std(rewards)\n",
    "    print(f\"  Mean reward: {mean:.2f} ± {std:.2f}\\n\")\n",
    "    \n",
    "    return rewards, video_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate all models\n",
    "print(\"=\"*60)\n",
    "print(\"EVALUATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "rewards_sac, video_sac = evaluate(model_sac, 'SAC')\n",
    "rewards_td3, video_td3 = evaluate(model_td3, 'TD3')\n",
    "rewards_ppo, video_ppo = evaluate(model_ppo, 'PPO')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show videos\n",
    "print(\"SAC:\")\n",
    "display(Video(video_sac, embed=True, width=600))\n",
    "\n",
    "print(\"\\nTD3:\")\n",
    "display(Video(video_td3, embed=True, width=600))\n",
    "\n",
    "print(\"\\nPPO:\")\n",
    "display(Video(video_ppo, embed=True, width=600))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparison plot\n",
    "algorithms = ['SAC', 'TD3', 'PPO']\n",
    "means = [np.mean(rewards_sac), np.mean(rewards_td3), np.mean(rewards_ppo)]\n",
    "stds = [np.std(rewards_sac), np.std(rewards_td3), np.std(rewards_ppo)]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "x = np.arange(len(algorithms))\n",
    "colors = ['#1f77b4', '#ff7f0e', '#2ca02c']\n",
    "\n",
    "bars = plt.bar(x, means, yerr=stds, align='center', alpha=0.8,\n",
    "               ecolor='black', capsize=10, color=colors)\n",
    "\n",
    "plt.xlabel('Algorithm', fontsize=12, fontweight='bold')\n",
    "plt.ylabel('Mean Reward', fontsize=12, fontweight='bold')\n",
    "plt.title('Algorithm Performance Comparison\\nBipedalWalker-v3', fontsize=14, fontweight='bold')\n",
    "plt.xticks(x, algorithms, fontsize=11)\n",
    "plt.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "for bar, mean, std in zip(bars, means, stds):\n",
    "    plt.text(bar.get_x() + bar.get_width()/2., bar.get_height(),\n",
    "            f'{mean:.1f}±{std:.1f}', ha='center', va='bottom',\n",
    "            fontsize=10, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('results/comparison.png', dpi=300)\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nResults:\")\n",
    "for alg, mean, std in zip(algorithms, means, stds):\n",
    "    print(f\"  {alg}: {mean:.2f} ± {std:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TensorBoard\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir tensorboard_logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download results\n",
    "!zip -r -q results.zip models/ videos/ results/ logs/\n",
    "\n",
    "from google.colab import files\n",
    "files.download('results.zip')\n",
    "\n",
    "print(\"\\nDone! Results saved in results.zip\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
